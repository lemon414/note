正则回顾

单字修饰符

数量修饰符

边界修饰符

分组修饰符



正则爬取站长之家站长素材高清图片情侣图片

```python
import urllib.request
import re

def create_request(page)
	base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
    if page == 1:
        url = base_url + ',html'
    else:
        url = base_url + '_' + str(page) + '.html'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
    }
    request = urllib.request.Requset(url=url,headers=headers)
    return request

def get_content(request):
    response = urllib.request.urlopen(url=url,headers=headers)
    content = response.read().decode('utf-8')
    return content

def down_load(content):
    pattern = re.compile('<div class="box picblock col3".*?src2="(.*?)" alt="(.*?)"><a/>',re.S)
    objs = pattern.findall(content)
    for obj in objs:
        suffix = obj[0].split('.')[-1]
        filename = './qinglvtupian/'+obj[1]+'.'+suffix
        urllib.request.urlretrieve(url=obj[0],filename=filename)
        
if __name__ == '__main__':
    start_page = int(imput('请输入起始页码'))
    end_page = int(input('请输入结束页码'))
    for page in range(start_page,end_page+1):
        request = create_request(page)
        response = get_response(request)
        down_load(content)
        
```

xpath

```python
xpath使用：
	注意：提前安装xpath插件
	1.安装lxml库      
			pip install lxml -i https://pypi.douban.com/simple
	2.导入lxml.etree  
			from lxml import etree
	3.etree.parse()   解析本地文件
		    html_tree = etree.parse('XX.html')	
	4.etree.HTML()    服务器响应文件
		    html_tree = etree.HTML(response.read().decode('utf-8')	
	4.html_tree.xpath(xpath路径)
```

xpath的基本语法

```
xpath基本语法：
	1.路径查询
		//：查找所有子孙节点，不考虑层级关系
		/ ：找直接子节点
	2.谓词查询
		//div[@id]  
		//div[@id="maincontent"]    
	3.属性查询
		//@class         
	4.模糊查询
		//div[contains(@id, "he")]   
		//div[starts-with(@id, "he")] 
	5.内容查询
		//div/h1/text()
	6.逻辑运算
		//div[@id="head" and @class="s_down"]
		//title | //price
```

```python
eg 
------------------------------------------------------------------
相配套的index.html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <title>Title</title>
</head>
<body>
    <div id="d1">
        <ul>
            <li class="l1">hen</li>
            <li>dog</li>
            <li>cat</li>
            <li>pig</li>
            <li>monky</li>
        </ul>
        <a href="http://www.baidu.com">zoo</a>
    </div>

    <div class="d2">
        <ul>
            <li id="l1">大连</li>
            <li id="l2">锦州</li>
            <li id="c1">成都</li>
            <li>大理</li>
            <li>日本</li>
        </ul>
        <a href="http://www.tuniu.com">location</a>
    </div>
</body>
</html>


------------------------------------------------------------------
# xpath可以解析本地文件
#   html_tree = etree.parse('XX.html')
# xpath可以解析服务器响应的文件  response.read().decode()
#   html_tree = etree.HTML(response.read().decode('utf-8')

#   html_tree.xpath(xpath语法)

from lxml import etree

# 本地文件中必须所有的标签都有结尾
tree = etree.parse('index.html')
#       //：查找所有子孙节点，不考虑层级关系
# 		/ ：找直接子节点
# a = tree.xpath('//div[@id="d1"]//li')
# print(a)

# 找到div标签中有id属性的
# a = tree.xpath('//div[@id]')
# print(a)

# 属性查询
# a = tree.xpath('//div[@id="d1"]/ul/li[1]/@class')
# print(a)

# a_list = tree.xpath('//div[@class="d2"]/ul/li[contains(@id,"l")]/@id')
# for a in a_list:
#     print(a)

# a = tree.xpath('//div[@class="d2"]/ul/li[starts-with(@id,"c")]/@id')
# print(a)

a = tree.xpath('//div[@id="d1"]/a/text()')
print(a)

```

xpath 爬取站长之家图片

```python
import urllib.request

from lxml import etree

start_page = int(input('请输入起始页码'))
end_page = int(input('请输入结束页码'))
for page in range(start_page, end_page + 1):
    base_url = 'http://sc.chinaz.com/tupian/gudianmeinvtupian'
    if page == 1:
        url = base_url + '.html'
    else:
        url = base_url + '_' + str(page) + '.html'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
    }
    request = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    html_tree = etree.HTML(content)
    src_list = html_tree.xpath('//div[@id="container"]//a/img/@src2')
    alt_list = html_tree.xpath('//div[@id="container"]//a/@alt')
    for i in range(len(src_list)):
        src = src_list[i]
        alt = alt_list[i]
        filename = './gudianmeinv/' + alt + '.jpg'
        urllib.request.urlretrieve(url=src, filename=filename)
```

jsonpath

```
jsonpath的安装及使用方式：
			pip安装： 
				   pip install jsonpath
			jsonpath的使用：
                    obj = json.load(open('json文件', 'r', encoding='utf-8'))
                    ret = jsonpath.jsonpath(obj, 'jsonpath语法')
```

json对象的转换

````
json对象的转换
	json.loads()
		是将字符串转化为python对象
	json.dumps()
		将python对象转化为json格式的字符串
	json.load()
		读取json格式的文本，转化为python对象
		json.load(open(a.json))
	json.dump()
		将python对象写入到文本中
````

教程连接（http://blog.csdn.net/luxideyao/article/details/77802389）

jsonpath 爬取智联招聘python岗位信息

```python
import urllib.reque
import jsonpath
import json

url = ''
headers = {}
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
with open('zhilianzhaopin.json','w',encoding='utf-8')as fp:
    fp.write(content)
    
obj = json.load(open('zhilianzhaopin.json','r',encoding='utf-8'))
jobname_list = jsonpath.jsonpath(obj,'$..jobName')
companyname_list = jsonpath.jsonpath(obj,'$..company.name')
salary_list = jsonpath.jsonpath(obj,'$..salary')

jobinfo = []
for i in range(len(jobname_list)):
    job = jobname_list[i]
    cname = campanyname_list[i]
    salary = salary_list[i]
    jobinfo={}
    jobinfo['job'] = job
    jobinfo['cname'] = cname
    jobinfo['salary'] = salary
    jobinfo_list.append(jobinfo)

with open('zhilianxinxi.json','w',encoding='utf-8')as fp:
    fp.write(str(jobinfo_list))
```

BeautifulSoup(bs4)

bs4默认打开文件编码是gbk

```python
a = soup.find('a',title='qf')
```







bs4 爬取股票

```python
import urllib.request
from bs4 import BeautifulSoup

url = 'http://quote.stockstar.com/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
}

request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('gb2312')
tr_list = soup.select('#datalist tr')
for tr in tr_list:
    code = tr.find_all('td')[0].get_text()
    name = tr.find_all('td')[1].get_text()
    price = tr.find_all('td')[2].get_text()
    print(code,name,price)
```



爬取 阳光宽频网

```python
服务器响应的数据不完整 用selenium
```











